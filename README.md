# ğŸ§  Job Enrichment Pipeline

A suppressorâ€‘aware, modular pipeline for deduplicating, filtering, enriching, and narrating infrastructureâ€‘aligned job data. Built for remoteâ€‘first teams and recruiter review.

## ğŸ‘‹ Overview

This project transforms noisy job batches into auditâ€‘safe, narratable datasets. It reflects my engineering philosophy: suppressorâ€‘aware clarity, schema hygiene, and markdown storytelling.

## ğŸ§© Modules at a Glance (Pipeline Stages)

| Module                  | Pipeline Stage         | Purpose                                                                 | Output                                     |
|-------------------------|-------------------------|-------------------------------------------------------------------------|---------------------------------------------|
| deduplication_stub.py   | Stage 1 â€” Hygiene       | Cleans encodings, patches corrupted text, suppresses malformed entries, deduplicates | deduped_batch.json, suppressed_batch.json |
| filter_entries.py       | Helper Library          | Roleâ€‘tag filters, techâ€‘stack filters, saturation checks, unfamiliarâ€‘tech suppressor | Used by benchmark_runner.py |
| benchmark_runner.py     | Stage 2 â€” Enrichment    | Applies filters, scores jobs and platforms, normalizes fields, narrates suppressions | benchmarked_batch.json |
| narrate_batch.py        | Stage 3 â€” Summary       | Generates batchâ€‘level markdown summary with score buckets and suppression breakdown | infra_batch_summary.md |
| sql_import.py           | Stage 4 â€” Persistence   | Loads enriched data into SQLite for querying and dashboard use          | jobs.db |
| md_exporter.py          | Stage 5 â€” Presentation  | Exports one markdown file per job and platform                          | job_markdowns/, platform_markdowns/ |
| run_pipeline.py         | Orchestrator            | Runs all pipeline stages in sequence, halting on failure                | Full pipeline output |

## ğŸš€ Running the Pipeline

- Take the role or job description you want to enrich.
- Use the current `rawfeed_batch.json` as a template (convert it using AI).
- Save the updated content into `rawfeed_batch.json` at the project root.
- Run:

```bash
python run_pipeline.py
```

## ğŸ§± Job Entry Format (JSON Schema)

Each job entry in `rawfeed_batch.json` follows this structure:

```json
{
  "job_id": "",
  "title": "",
  "company": "",
  "location": "",
  "wage_band": "",
  "source": "",
  "suppression_reason": null,
  "date_posted": null,
  "days_listed": null,
  "level": "",
  "tech_stack": [],
  "role_tags": [],
  "description_length": 0,
  "link_count": 0,
  "notes": ""
}
```

### Field Notes
- **job_id** â€” unique identifier for the job  
- **wage_band** â€” optional; used for wage tagging  
- **suppression_reason** â€” null unless filtered out  
- **tech_stack** â€” list of technologies extracted or inferred  
- **role_tags** â€” qualitative tags used for scoring  
- **description_length** â€” character count of the job description  
- **notes** â€” qualitative summary used for narration

## ğŸ“¡ Platform Intelligence Profiles

In addition to jobâ€‘level enrichment, the pipeline supports platformâ€‘level intelligence.  
These entries are not scraped from APIs â€” they are qualitative, suppressorâ€‘aware evaluations of job platforms based on:

- role density
- schema consistency
- techâ€‘stack transparency
- suppressor saturation
- contract hygiene
- platform behavior and drift patterns

## ğŸ§± Platform Entry Format (JSON Schema)

Each platform entry in rawfeed_batch.json follows this structure:


```json
{
  "platform_id": "",
  "name": "",
  "website": "",
  "platform_type": "",
  "infra_role_density": "",
  "tooling_transparency": "",
  "contract_hygiene": "",
  "suppressor_saturation": "",
  "tech_stack_coverage": [],
  "schema_drift_notes": "",
  "ideal_for": [],
  "notes": ""
}
```

### Field Notes
- **platform_type** â€” classification of the platform (job board, talent marketplace, staffing aggregator)
- **infra_role_density** â€” qualitative measure of how often infraâ€‘aligned roles appear
- **tooling_transparency** â€” how clearly the platform exposes tech stacks in listings
- **contract_hygiene** â€” clarity and honesty of contract terms and employment structure
- **suppressor_saturation** â€” how noisy, spamâ€‘heavy, or lowâ€‘signal the platform tends to be
- **ideal_for** â€” recommended use cases (e.g., earlyâ€‘career sourcing, infraâ€‘heavy roles, contractâ€‘first markets)
- **schema_drift_notes** â€” observations about field inconsistencies or platformâ€‘level drift

Platform profiles allow the pipeline to:

- benchmark platforms alongside jobs
- narrate platformâ€‘level suppressions
- generate markdown summaries for recruiterâ€‘ready review
- support future agentic workflows (platform selection, sourcing strategy, noiseâ€‘aware job scouting)

Platform entries live in rawfeed_batch.json alongside job entries and flow through the same enrichment and narration stages.

## ğŸ§­ How to Generate Platform Intelligence Profiles

Platform profiles are qualitative evaluations, not scraped data.  
They are created by prompting an AI model to analyze a job platform based on:

- role density
- schema consistency
- techâ€‘stack transparency
- suppressor saturation
- contract hygiene
- platform behavior and drift patterns

Prompt to generate a new platform entry (shown without code fences):

Generate a Platform Intelligence Profile for <PLATFORM_NAME> using the JSON schema defined in the README. Base your evaluation on qualitative signals such as role density, schema consistency, tech-stack transparency, suppressor saturation, contract hygiene, and platform behavior. Return ONLY valid JSON that matches the schema.

After generating the JSON, paste the platform entry directly into rawfeed_batch.json alongside job entries. The enrichment pipeline will benchmark platforms and jobs together.

## ğŸ“¦ Outputs

- JSON: deduplicated, suppressed, and enriched batches
- Markdown: recruiterâ€‘friendly summaries and individual entries
- SQLite: structured querying and dashboardâ€‘ready data
- All artifacts appear directly in the project directory

## ğŸ§¼ Design Philosophy

- Suppressorâ€‘aware filtering and narration
- Modular batch hygiene and techâ€‘fluency benchmarking
- Markdown storytelling for recruiter clarity
- SQLiteâ€‘backed persistence for auditâ€‘safe review

## ğŸ§  Use Cases

- Cleaning and enriching jobâ€‘market datasets
- Producing recruiterâ€‘ready summaries from noisy sources
- Demonstrating suppressorâ€‘aware filtering and auditâ€‘safe data pipelines
- Prototyping GenAIâ€‘ready ingestion and enrichment workflows

## ğŸ”® Roadmap

- ONET title inference improvements
- Wage tagging enhancements
- Multiâ€‘source ingestion (HN, WWR, etc.)
- Dashboard layer for batch insights
- Agentic jobâ€‘matching workflow

## ğŸ› ï¸ Tech Stack

- Python Â· JSON Â· Markdown Â· SQLite
- difflib, re, datetime, os, collections

---

ğŸ§  Built by Chad â€” diagnostic architect and workflow engineer  
ğŸ¯ Modularizing enrichment pipelines for infraâ€‘aligned clarity  
ğŸ“ GitHubâ€‘ready for recruiter review and remoteâ€‘first roles
