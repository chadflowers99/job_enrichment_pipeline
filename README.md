# ğŸ§  Job Enrichment Pipeline

A suppressorâ€‘aware, modular pipeline for deduplicating, filtering, enriching, and narrating infrastructureâ€‘aligned job data. Built for remoteâ€‘first teams and recruiter review.

## ğŸ‘‹ Overview

This project transforms noisy job batches into auditâ€‘safe, narratable datasets. It reflects my engineering philosophy: suppressorâ€‘aware clarity, schema hygiene, and markdown storytelling.

## ğŸ§© Modules at a Glance (Pipeline Stages)

| Module                 | Pipeline Stage        | Purpose                                                                 | Output                                   |
|------------------------|------------------------|-------------------------------------------------------------------------|-------------------------------------------|
| `deduplication_stub.py` | **Stage 1 â€” Hygiene**     | Cleans encodings, patches corrupted text, suppresses malformed entries, deduplicates | `deduped_batch.json`, `suppressed_batch.json` |
| `filter_entries.py`     | **Helper Library**        | Roleâ€‘tag filters, techâ€‘stack filters, saturation checks, unfamiliarâ€‘tech suppressor | Used by `benchmark_runner.py` |
| `benchmark_runner.py`   | **Stage 2 â€” Enrichment**  | Applies filters, scores jobs and platforms, normalizes fields, narrates suppressions | `benchmarked_batch.json` |
| `narrate_batch.py`      | **Stage 3 â€” Summary**     | Generates batchâ€‘level markdown summary with score buckets and suppression breakdown | `infra_batch_summary.md` |
| `sql_import.py`         | **Stage 4 â€” Persistence** | Loads enriched data into SQLite for querying and dashboard use          | `jobs.db` |
| `md_exporter.py`        | **Stage 5 â€” Presentation**| Exports one markdown file per job and platform                          | `job_markdowns/`, `platform_markdowns/` |
| `run_pipeline.py`       | **Orchestrator**          | Runs all pipeline stages in sequence, halting on failure                | Full pipeline output |

ğŸš€ Running the Pipeline  
- Take the role or job description you want to enrich.  
- Use the current rawfeed_batch.json as a template (convert it using AI).  
- Save the updated content into rawfeed_batch.json at the project root.  
- Run:  
    python run_pipeline.py

## ğŸ“¡ Platform Intelligence Profiles

In addition to jobâ€‘level enrichment, the pipeline supports platformâ€‘level intelligence.  
These entries are not scraped from APIs â€” they are qualitative, suppressorâ€‘aware evaluations of job platforms based on:  
- role density  
- schema consistency  
- techâ€‘stack transparency  
- suppressor saturation  
- contract hygiene  
- platform behavior and drift patterns  

Each platform is represented as a structured JSON object:

```json
{
  "platform_id": "",
  "name": "",
  "website": "",
  "platform_type": "",
  "infra_role_density": "",
  "tooling_transparency": "",
  "contract_hygiene": "",
  "suppressor_saturation": "",
  "tech_stack_coverage": [],
  "schema_drift_notes": "",
  "ideal_for": [],
  "notes": ""
}
```

Platform profiles allow the pipeline to:  
- benchmark platforms alongside jobs  
- narrate platformâ€‘level suppressions  
- generate markdown summaries for recruiterâ€‘ready review  
- support future agentic workflows (e.g., platform selection, sourcing strategy, noiseâ€‘aware job scouting)  

Platform entries live in rawfeed_batch.json alongside job entries and flow through the same enrichment and narration stages.

## ğŸ“¦ Outputs

- JSON: deduplicated, suppressed, and enriched batches  
- Markdown: recruiterâ€‘friendly summaries and individual entries  
- SQLite: structured querying and dashboardâ€‘ready data  
- All artifacts appear directly in the project directory  

## ğŸ§¼ Design Philosophy

- Suppressorâ€‘aware filtering and narration  
- Modular batch hygiene and techâ€‘fluency benchmarking  
- Markdown storytelling for recruiter clarity  
- SQLiteâ€‘backed persistence for auditâ€‘safe review  

## ğŸ§  Use Cases

- Cleaning and enriching jobâ€‘market datasets  
- Producing recruiterâ€‘ready summaries from noisy sources  
- Demonstrating suppressorâ€‘aware filtering and auditâ€‘safe data pipelines  
- Prototyping GenAIâ€‘ready ingestion and enrichment workflows  

## ğŸ”® Roadmap

- ONET title inference improvements  
- Wage tagging enhancements  
- Multiâ€‘source ingestion (HN, WWR, etc.)  
- Dashboard layer for batch insights  
- Agentic jobâ€‘matching workflow  

## ğŸ› ï¸ Tech Stack

- Python Â· JSON Â· Markdown Â· SQLite  
- `difflib`, `re`, `datetime`, `os`, `collections`  

---

ğŸ§  Built by Chad â€” diagnostic architect and workflow engineer  
ğŸ¯ Modularizing enrichment pipelines for infraâ€‘aligned clarity  
ğŸ“ GitHubâ€‘ready for recruiter review and remoteâ€‘first roles
